{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_open_source = \"/home/neemias/PerceptSent-LLM-approach/data/\"\n",
    "data_path_open_ai = \"/home/neemias/PerceptSent-LLM-approach/data/gpt4-openai-classify/\"\n",
    "data_paths = [os.path.join(data_path_open_source, f) \n",
    "              for f in os.listdir(data_path_open_source) if f.endswith('.csv')] + [os.path.join(data_path_open_ai, f)\n",
    "                                                              for f in os.listdir(data_path_open_ai) if f.endswith('.csv')]\n",
    "del data_paths[6]\n",
    "data_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/neemias/PerceptSent-LLM-approach/data/percept_dataset_alpha5_p5.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt(row):\n",
    "    return f\"{row['text']} - {row['sentiment']}\"\n",
    "\n",
    "def prepare_data(dataframe: pd.DataFrame):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "\n",
    "    # X_train, X_test = train_test_split(\n",
    "    #     dataframe,\n",
    "    #     test_size=0.2,\n",
    "    #     random_state=42\n",
    "    # )\n",
    "    for sentiment in [0, 1, 2, 3, 4]:\n",
    "        \n",
    "        train, test = train_test_split(df[df.sentiment == sentiment],\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=42)\n",
    "        X_train.append(train)\n",
    "        X_test.append(test)\n",
    "\n",
    "    X_train = pd.concat(X_train)\n",
    "    X_test = pd.concat(X_test)\n",
    "\n",
    "\n",
    "    prompt = \"\"\"What is the sentiment of this description? Please choose an answer from \n",
    "        {\n",
    "            \"Positive\": 4,\n",
    "            \"SlightlyPositive\": 3,\n",
    "            \"Neutral\": 2,\n",
    "            \"SlightlyNegative\": 1,\n",
    "            \"Negative\": 0\n",
    "        }\n",
    "    \"\"\"\n",
    "    #         {Positive/SlightlyPositive/Neutral/SlightlyNegative/Negative}.\n",
    "    # sentiment = {\n",
    "    #     4: \"Positive\",\n",
    "    #     3: \"SlightlyPositive\",\n",
    "    #     2: \"Neutral\",\n",
    "    #     1: \"SlightlyNegative\",\n",
    "    #     0: \"Negative\",\n",
    "    # }\n",
    "    # X_train[\"sentiment\"] = X_train[\"sentiment\"].apply(lambda x: sentiment[x])\n",
    "    # X_test[\"sentiment\"] = X_test[\"sentiment\"].apply(lambda x: sentiment[x])\n",
    "    # print(len(X_train), len(X_test))\n",
    "    X_train[\"input\"] = X_train[\"text\"]\n",
    "    X_test[\"input\"] = X_test[\"text\"]\n",
    "\n",
    "    X_train[\"text\"] = X_train[[\"text\", \"sentiment\"]].apply(lambda x: prompt+x[\"text\"]+'='+str(x[\"sentiment\"]), axis=1)\n",
    "    # X_test[\"text\"] = X_test[[\"text\", \"sentiment\"]].apply(lambda x: prompt+x[\"text\"]+'='+str(x[\"sentiment\"]), axis=1)\n",
    "    # X_train[\"text\"] = X_train[[\"text\"]].apply(lambda x: prompt+x+'=', axis=1)\n",
    "    X_test[\"text\"] = X_test[[\"text\"]].apply(lambda x: prompt+x+'=', axis=1)\n",
    "\n",
    "    train_data = Dataset.from_pandas(X_train)#.iloc[0:500])\n",
    "    test_data = Dataset.from_pandas(X_test)\n",
    "\n",
    "    return train_data, test_data, X_test.drop([\"sentiment\"], axis=1), X_test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, X_test, y_test = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_model(model_name=\"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"):\n",
    "# def init_model(model_name=\"meta-llama/Llama-3.2-1B\"):\n",
    "# def init_model(model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "def init_model(model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"):\n",
    "# def init_model(model_name=\"nvidia/NV-Embed-v2\"):\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    #     bnb_4bit_compute_dtype=torch.float16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraModel, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    f1_metric = evaluate.load(\"f1\", use_auth_token=False)\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    return {\"f1\": f1[\"f1\"]}\n",
    "\n",
    "def train(model, tokenizer, train_data, eval_data):\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        # lora_alpha=2,\n",
    "        # lora_dropout=0.1,\n",
    "        # r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"../logs/log\",\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=0,\n",
    "        logging_steps=25,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.001,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        gradient_checkpointing=True,\n",
    "        eval_accumulation_steps=2,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        # compute_metrics=compute_metrics,\n",
    "        packing=False,\n",
    "        max_seq_length=1064,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    output_dir = \"../logs/results/trained_model\"\n",
    "\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, tokenizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def inference(pipe, prompt):\n",
    "    result = pipe(prompt)\n",
    "    # print(result)\n",
    "    # print()\n",
    "    answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "    return answer\n",
    "\n",
    "def predict(X_test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    pipe = pipeline(task=\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=1,\n",
    "                    temperature=0.01,\n",
    "                    do_sample=True\n",
    "                    # device='cuda'\n",
    "                    )\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        answer = inference(pipe, prompt)\n",
    "        # print(answer)\n",
    "        y_pred.append(int(answer))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = {\n",
    "    \"Positive\": 4,\n",
    "    \"SlightlyPositive\": 3,\n",
    "    \"Neutral\": 2,\n",
    "    \"SlightlyNegative\": 1,\n",
    "    \"Negative\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "print(f\"{f1_score(y_true=[sentiment[v] for v in y_test.to_list()], y_pred=[sentiment[v] for v in y_pred], average='weighted'):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
