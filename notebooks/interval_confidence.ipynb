{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load F1-score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of experiments: 2\n"
     ]
    }
   ],
   "source": [
    "# root_dir = \"../experiments\" # finetuning\n",
    "# root_dir = \"../experiments-not-finetuning\" # pre-trained\n",
    "root_dir = \"../experiments-finetuning\"\n",
    "# root_dir = \"../experiments-twitter\" # \n",
    "# root_dir = \"../experiments-swin\"\n",
    "data_paths = []\n",
    "\n",
    "for dirs in os.listdir(root_dir):\n",
    "\n",
    "    # if not dirs.split('-')[0] in (\"openai\", \"deepseek\"):\n",
    "    if dirs.split('-')[0] == \"deepseek\" and dirs.split('-')[1] == \"llama3\":\n",
    "        # print(dirs.split('-')[0])\n",
    "        data_paths.append(os.path.join(root_dir, dirs+\"/logs/test_logs.csv\"))\n",
    "        # print(dirs)\n",
    "        # break\n",
    "\n",
    "print(f\"Amount of experiments: {len(data_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Problem: deepseek-llama3-qlora-p3-alpha5\n",
      "Average F1-score: 89.29%\n",
      "Time: 6.67 hours\n",
      "Inteval: 0.42%\n",
      "\n",
      "\n",
      "Problem: deepseek-llama3-qlora-p5-alpha5\n",
      "Average F1-score: 69.79%\n",
      "Time: 2.97 hours\n",
      "Inteval: 2.31%\n"
     ]
    }
   ],
   "source": [
    "for data_path in sorted(data_paths):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    f1_scores = df[\"f1_score\"].to_numpy()\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_acc = np.mean(df[\"accuracy\"].to_numpy())\n",
    "    # define the confidence level\n",
    "    confidence_level = 0.95\n",
    "    degrees_freedon = len(f1_scores)-1\n",
    "\n",
    "    confidence_interval = stats.t.interval(\n",
    "        confidence_level, \n",
    "        degrees_freedon, \n",
    "        loc=mean_f1, \n",
    "        scale=stats.sem(f1_scores)\n",
    "    )\n",
    "\n",
    "    if (data_path.split('/')[-3].split('.')[0][-1] in (\"3\", \"5\") and data_path.split('/')[-3].split('.')[0].split('-')[-2][-1] in (\"3\", \"5\")):\n",
    "        print(f\"\\n\\nProblem: {data_path.split('/')[-3].split('.')[0]}\")\n",
    "\n",
    "        # print(f\"Max F1-score: {max(f1_scores)}\")\n",
    "        print(f\"Average F1-score: {mean_f1*100:.2f}%\")\n",
    "        print(f\"Time: {df['time'].to_numpy().sum()/3600:.2f} hours\")\n",
    "        # print(f\"Average Acc: {mean_acc*100:.2f}%\")\n",
    "        # print(f\"Median F1-score: {np.median(f1_scores)}\")\n",
    "        # print(f\"Confidence interval 95%: {confidence_interval}\")\n",
    "        # print(f\"Inteval: {abs(confidence_interval[0]-mean_f1)*100:.2f}%\")\n",
    "        print(f\"Inteval: {df['f1_score'].to_numpy().std()*100:.2f}%\")\n",
    "        # print(f\"Inteval: {abs(confidence_interval[0]-mean_acc)*100:.2f}%\")\n",
    "        confidence_interval = stats.t.interval(\n",
    "            confidence_level, \n",
    "            degrees_freedon, \n",
    "            loc=max(f1_scores), \n",
    "            scale=stats.sem(f1_scores)\n",
    "        )\n",
    "\n",
    "        # print(f\"Inteval: {abs(confidence_interval[0]-max(f1_scores))} - Interval: {abs(confidence_interval[1]-max(f1_scores))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{2.97:.1f}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
