experiment_name: "Experiment using Distil-BERT"
learning_rate: 1e-5
batch_size: 64
epochs: 1
model_path: "distilbert-base-uncased"
model_name: "distil-bert"
max_len: 512
log_dir: "experiments-not-finetuning/openai-distil-experiment-p5-alpha3/logs"
checkpoint_dir: "checkpoints"
